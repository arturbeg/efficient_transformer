Warning: Traceback of forward call that caused the error:
  File "main.py", line 215, in <module>
    train(train_data=train_data)
  File "main.py", line 177, in train
    output, aux_loss = model(src=None, trg=data, src_mask=None, trg_mask=trg_mask, is_lm=True)
  File "/jmain01/home/JAD029/jph13/axb14-jph13/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/jmain01/home/JAD029/jph13/axb14-jph13/mog/mog_transformer/playground/Models.py", line 71, in forward
    output = F.log_softmax(output, dim=-1)  # along the embedding (d_model) dimension
  File "/jmain01/home/JAD029/jph13/axb14-jph13/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 1317, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /opt/conda/conda-bld/pytorch_1579022071458/work/torch/csrc/autograd/python_anomaly_mode.cpp:57)
Gating function is:  moe
Running without errors
| epoch   1 |   200/  466 batches | lr 2.00 | ms/batch 1350.62 | loss  9.70 | aux_loss  0.01 | ppl 16241.77
| epoch   1 |   400/  466 batches | lr 2.00 | ms/batch 1319.87 | loss  6.93 | aux_loss  0.01 | ppl  1017.40
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 623.77s | valid loss  6.34 | valid ppl   567.31
-----------------------------------------------------------------------------------------
Running without errors
| epoch   2 |   200/  466 batches | lr 2.00 | ms/batch 1301.86 | loss  6.44 | aux_loss  0.01 | ppl   625.72
| epoch   2 |   400/  466 batches | lr 2.00 | ms/batch 1296.71 | loss  6.18 | aux_loss  0.01 | ppl   481.92
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 610.29s | valid loss  5.91 | valid ppl   367.20
-----------------------------------------------------------------------------------------
Running without errors
| epoch   3 |   200/  466 batches | lr 2.00 | ms/batch 1318.98 | loss  5.91 | aux_loss  0.01 | ppl   368.86
| epoch   3 |   400/  466 batches | lr 2.00 | ms/batch 1310.87 | loss  5.73 | aux_loss  0.01 | ppl   308.21
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 615.75s | valid loss  5.67 | valid ppl   289.24
-----------------------------------------------------------------------------------------
Running without errors
| epoch   4 |   200/  466 batches | lr 2.00 | ms/batch 1322.90 | loss  5.54 | aux_loss  0.01 | ppl   255.30
| epoch   4 |   400/  466 batches | lr 2.00 | ms/batch 1298.95 | loss  5.40 | aux_loss  0.01 | ppl   222.02
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 614.63s | valid loss  5.54 | valid ppl   254.71
-----------------------------------------------------------------------------------------
Running without errors
Traceback (most recent call last):
  File "main.py", line 215, in <module>
    train(train_data=train_data)
  File "main.py", line 181, in train
    final_loss.backward()
  File "/jmain01/home/JAD029/jph13/axb14-jph13/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/jmain01/home/JAD029/jph13/axb14-jph13/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
srun: error: dgj324: task 0: Exited with exit code 1
