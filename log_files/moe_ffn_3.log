INFO:root:./model_files/moe_ffn_3.pt
INFO:root:The batch size is: 64
INFO:root:Number of epochs is : 2
INFO:root:The context length is : 128
INFO:root:D_model is : 256
INFO:root:Number of attention heads is : 4
INFO:root:Number of decoder layers is : 3
INFO:root:Initial learning rate is : 1.0
INFO:root:Number of warmup steps is : 4000
INFO:root:k is : 2
INFO:root:Number of experts is : 256
INFO:root:Type of gating used : FNN gating
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split train, from ./subwords32k/lm1b/subwords32k/1.0.0
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split test, from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:root:Gating function is: none
INFO:root:The optimizer used is: adam
INFO:root:Current Epoch is: 1
INFO:root:Running without errors
INFO:root:Learning rate has changed to: 3.186982954388445e-05
INFO:root:| epoch   1 | batch   128 | ms/batch 15795.95 | loss    10.5993 | aux_loss    13.1370 | ppl 40107.2682
INFO:root:Learning rate has changed to: 6.349260614556825e-05
INFO:root:| epoch   1 | batch   256 | ms/batch 15479.35 | loss     9.9422 | aux_loss    13.0569 | ppl 20789.1188
INFO:root:Learning rate has changed to: 9.511538274725204e-05
INFO:root:| epoch   1 | batch   384 | ms/batch 15464.97 | loss     8.0262 | aux_loss    13.0282 | ppl  3060.0623
INFO:root:Learning rate has changed to: 0.00012673815934893584
INFO:root:| epoch   1 | batch   512 | ms/batch 15375.96 | loss     7.3408 | aux_loss    13.0451 | ppl  1542.0139
INFO:root:Learning rate has changed to: 0.00015836093595061963
INFO:root:| epoch   1 | batch   640 | ms/batch 15429.12 | loss     7.1565 | aux_loss    13.0078 | ppl  1282.3574
INFO:root:Learning rate has changed to: 0.00018998371255230342
INFO:root:| epoch   1 | batch   768 | ms/batch 15388.58 | loss     6.9885 | aux_loss    13.0206 | ppl  1084.1170
INFO:root:Learning rate has changed to: 0.00022160648915398723
INFO:root:| epoch   1 | batch   896 | ms/batch 15350.28 | loss     6.8673 | aux_loss    13.0252 | ppl   960.3558
INFO:root:Learning rate has changed to: 0.000253229265755671
INFO:root:| epoch   1 | batch  1024 | ms/batch 15370.46 | loss     6.7581 | aux_loss    13.0137 | ppl   861.0249
INFO:root:Learning rate has changed to: 0.0002848520423573548
INFO:root:| epoch   1 | batch  1152 | ms/batch 15415.74 | loss     6.6461 | aux_loss    12.9980 | ppl   769.8102
INFO:root:Learning rate has changed to: 0.0003164748189590386
INFO:root:| epoch   1 | batch  1280 | ms/batch 15395.38 | loss     6.5613 | aux_loss    12.9994 | ppl   707.1661
INFO:root:Learning rate has changed to: 0.0003480975955607224
INFO:root:| epoch   1 | batch  1408 | ms/batch 15413.38 | loss     6.4757 | aux_loss    12.9803 | ppl   649.1901
INFO:root:Learning rate has changed to: 0.0003797203721624062
INFO:root:| epoch   1 | batch  1536 | ms/batch 15440.57 | loss     6.3920 | aux_loss    12.9582 | ppl   597.0634
INFO:root:Learning rate has changed to: 0.00041134314876409
INFO:root:| epoch   1 | batch  1664 | ms/batch 15420.00 | loss     6.3087 | aux_loss    12.9920 | ppl   549.3231
INFO:root:Learning rate has changed to: 0.0004429659253657738
INFO:root:| epoch   1 | batch  1792 | ms/batch 15357.47 | loss     6.2250 | aux_loss    12.9663 | ppl   505.2130
INFO:root:Learning rate has changed to: 0.0004745887019674576
INFO:root:| epoch   1 | batch  1920 | ms/batch 14936.71 | loss     6.1556 | aux_loss    12.9729 | ppl   471.3484
INFO:root:Learning rate has changed to: 0.0005062114785691414
INFO:root:| epoch   1 | batch  2048 | ms/batch 13892.24 | loss     6.1071 | aux_loss    12.9837 | ppl   449.0522
INFO:root:Learning rate has changed to: 0.0005378342551708252
INFO:root:| epoch   1 | batch  2176 | ms/batch 13626.47 | loss     6.0465 | aux_loss    12.9841 | ppl   422.6279
INFO:root:Learning rate has changed to: 0.000569457031772509
INFO:root:| epoch   1 | batch  2304 | ms/batch 13579.42 | loss     5.9962 | aux_loss    12.9704 | ppl   401.9011
INFO:root:Learning rate has changed to: 0.0006010798083741928
INFO:root:| epoch   1 | batch  2432 | ms/batch 13590.46 | loss     5.9431 | aux_loss    12.9730 | ppl   381.1298
INFO:root:Learning rate has changed to: 0.0006327025849758765
INFO:root:| epoch   1 | batch  2560 | ms/batch 13806.33 | loss     5.9128 | aux_loss    12.9660 | ppl   369.7426
INFO:root:Learning rate has changed to: 0.0006643253615775603
INFO:root:| epoch   1 | batch  2688 | ms/batch 13723.16 | loss     5.8759 | aux_loss    12.9683 | ppl   356.3573
INFO:root:Learning rate has changed to: 0.0006959481381792441
INFO:root:| epoch   1 | batch  2816 | ms/batch 13610.88 | loss     5.8518 | aux_loss    12.9777 | ppl   347.8562
INFO:root:Learning rate has changed to: 0.000727570914780928
INFO:root:| epoch   1 | batch  2944 | ms/batch 13682.61 | loss     5.8225 | aux_loss    12.9774 | ppl   337.8013
INFO:root:Learning rate has changed to: 0.0007591936913826118
INFO:root:| epoch   1 | batch  3072 | ms/batch 13575.18 | loss     5.7852 | aux_loss    12.9683 | ppl   325.4359
INFO:root:Learning rate has changed to: 0.0007908164679842956
INFO:root:| epoch   1 | batch  3200 | ms/batch 13599.33 | loss     5.7567 | aux_loss    12.9643 | ppl   316.2904
INFO:root:Learning rate has changed to: 0.0008224392445859793
INFO:root:| epoch   1 | batch  3328 | ms/batch 13732.10 | loss     5.7370 | aux_loss    12.9550 | ppl   310.1334
INFO:root:Learning rate has changed to: 0.0008540620211876631
INFO:root:| epoch   1 | batch  3456 | ms/batch 13837.41 | loss     5.7145 | aux_loss    12.9690 | ppl   303.2292
INFO:root:Learning rate has changed to: 0.0008856847977893469
INFO:root:| epoch   1 | batch  3584 | ms/batch 13818.54 | loss     5.6942 | aux_loss    12.9376 | ppl   297.1271
INFO:root:Learning rate has changed to: 0.0009173075743910307
INFO:root:| epoch   1 | batch  3712 | ms/batch 13646.89 | loss     5.6752 | aux_loss    12.9330 | ppl   291.5428
INFO:root:Learning rate has changed to: 0.0009489303509927146
INFO:root:| epoch   1 | batch  3840 | ms/batch 13636.80 | loss     5.6429 | aux_loss    12.9169 | ppl   282.2737
INFO:root:Learning rate has changed to: 0.0009805531275943983
INFO:root:| epoch   1 | batch  3968 | ms/batch 13706.23 | loss     5.6293 | aux_loss    12.9285 | ppl   278.4692
INFO:root:Learning rate has changed to: 0.0009764433125338821
INFO:root:| epoch   1 | batch  4096 | ms/batch 13698.50 | loss     5.6168 | aux_loss    12.9132 | ppl   275.0066
INFO:root:Learning rate has changed to: 0.0009615384615384616
INFO:root:| epoch   1 | batch  4224 | ms/batch 13899.75 | loss     5.6002 | aux_loss    12.9154 | ppl   270.4711
INFO:root:Learning rate has changed to: 0.0009472959569955596
INFO:root:| epoch   1 | batch  4352 | ms/batch 13839.91 | loss     5.5763 | aux_loss    12.8949 | ppl   264.0871
INFO:root:Learning rate has changed to: 0.0009336681528218903
INFO:root:| epoch   1 | batch  4480 | ms/batch 13772.09 | loss     5.5548 | aux_loss    12.8571 | ppl   258.4881
INFO:root:Learning rate has changed to: 0.0009206120672864873
INFO:root:| epoch   1 | batch  4608 | ms/batch 13783.80 | loss     5.5399 | aux_loss    12.8567 | ppl   254.6476
INFO:root:Learning rate has changed to: 0.0009080888118835636
INFO:root:| epoch   1 | batch  4736 | ms/batch 13805.24 | loss     5.5210 | aux_loss    12.8677 | ppl   249.8911
INFO:root:Learning rate has changed to: 0.0008960631034158175
INFO:root:| epoch   1 | batch  4864 | ms/batch 13810.14 | loss     5.5014 | aux_loss    12.8538 | ppl   245.0254
INFO:root:Learning rate has changed to: 0.0008845028453299376
INFO:root:| epoch   1 | batch  4992 | ms/batch 13825.73 | loss     5.4823 | aux_loss    12.8445 | ppl   240.3949
