INFO:root:./model_files/1591641174.893142_model_vanilla_transformer.pt
INFO:root:The batch size is: 256
INFO:root:Number of epochs is : 2
INFO:root:The context length is : 256
INFO:root:D_model is : 512
INFO:root:Number of attention heads is : 4
INFO:root:Number of decoder layers is : 3
INFO:root:Initial learning rate is : 0.01
INFO:root:Number of warmup steps is : 4000
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split train, from ./subwords32k/lm1b/subwords32k/1.0.0
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split test, from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:root:Gating function is: none
INFO:root:The optimizer used is: adam
