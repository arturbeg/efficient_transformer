INFO:root:./model_files/2020_07_11_01_51_15.pt
INFO:root:The batch size is: 64
INFO:root:Number of epochs is : 2
INFO:root:The context length is : 128
INFO:root:D_model is : 256
INFO:root:Number of attention heads is : 4
INFO:root:Number of decoder layers is : 3
INFO:root:Initial learning rate is : 1.0
INFO:root:Number of warmup steps is : 4000
INFO:root:k is : 2
INFO:root:Number of experts is : 1024
INFO:root:Type of gating used : FNN gating
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split train, from ./subwords32k/lm1b/subwords32k/1.0.0
WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)
INFO:absl:Load dataset info from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:absl:Reusing dataset lm1b (./subwords32k/lm1b/subwords32k/1.0.0)
INFO:absl:Constructing tf.data.Dataset for split test, from ./subwords32k/lm1b/subwords32k/1.0.0
INFO:root:Gating function is: none
INFO:root:The optimizer used is: adam
INFO:root:Current Epoch is: 1
