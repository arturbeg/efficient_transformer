INFO:root:./model_files/1586884231.44895_model_moe_transformer.pt
INFO:root:The batch size is: 32
INFO:root:Number of epochs is : 40
INFO:root:The context length is : 32
INFO:root:D_model is : 512
INFO:root:Number of attention heads is : 2
INFO:root:Number of decoder layers is : 6
INFO:root:Initial learning rate is : 0.0
INFO:root:Number of warmup steps is : 4000
INFO:root:Loading cached dataset...
INFO:root:Gating function is: moe
INFO:root:The optimizer used is: adam
